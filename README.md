# Medical Text Simplification using Fine-Tuned T5

This repository contains the implementation of a medical text simplification system that transforms complex medical content into plain language accessible to non-experts. The project fine-tunes the T5-small transformer model on the Cochrane Simplification dataset to generate simplified versions of medical text while preserving critical information.

![Medical Text Simplification Demo](demo_screenshot.PNG)

## Project Overview

Medical terminology can be a significant barrier to patient understanding. This project aims to bridge that gap by:

1. Fine-tuning a pre-trained T5 model on medical text simplification
2. Comparing multiple training configurations to identify optimal parameters
3. Evaluating model performance using BLEU and ROUGE-L metrics
4. Providing a user-friendly interface for simplifying medical text

## Repository Structure

```
.
├── data/
│   ├── train.json         # Training dataset
│   ├── validation.json    # Validation dataset
│   └── test.json          # Test dataset
├── results/
│   ├── baseline_model/    # Saved baseline model
│   ├── config_1/          # Saved model with config 1
│   ├── config_2/          # Saved model with config 2
│   ├── config_3/          # Saved model with config 3
│   ├── eval_detailed.csv  # Detailed evaluation results
│   └── eval_summary.csv   # Summary of evaluation metrics
├── train.py               # Model training script
├── evaluate.py            # Evaluation script
├── app.py                 # Streamlit web application
├── requirements.txt       # Required Python packages
├── README.md              # Project documentation
└── Technical_Report.pdf   # Detailed project report
```

## Setup Instructions

### Prerequisites

- Python 3.7+
- PyTorch 1.9+
- CUDA-compatible GPU (recommended)

### Installation

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/medical-text-simplification.git
   cd medical-text-simplification
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install the required packages:
   ```
   pip install -r requirements.txt
   ```

### Dataset Preparation

The project uses the Cochrane Simplification dataset from the GEM benchmark. The data is pre-processed and provided in the `data/` directory in JSON format.

- Each entry contains a complex medical text (`source`) and its simplified version (`target`)
- Train/validation/test splits are provided separately

## Usage

### Training Models

To train all model configurations:

```
python train.py
```

This will train the model with three different hyperparameter configurations and save each model in the `results/` directory.

### Evaluating Models

To evaluate all trained models on the test set:

```
python evaluate.py
```

This will generate:
- `results/eval_detailed.csv`: Detailed results for each test example
- `results/eval_summary.csv`: Average metrics for each model configuration

### Interactive Demo

To launch the Streamlit web application:

```
streamlit run app.py
```

This will start a local web server where you can:
- Input any medical text
- View simplified versions generated by each model
- Compare model outputs side-by-side
- Explore evaluation metrics and error analysis

## Model Configurations

We experimented with three different fine-tuning configurations:

| Configuration | Learning Rate | Batch Size | Epochs | Description |
|--------------|---------------|------------|--------|-------------|
| Baseline     | -             | -          | -      | Original T5-small without fine-tuning |
| Config 1     | 2e-5          | 4          | 3      | Conservative learning rate and batch size |
| Config 2     | 3e-5          | 8          | 4      | Larger batch, more aggressive training |
| Config 3     | 1e-4          | 8          | 2      | Fastest training, highest learning rate |

## Results

Based on our evaluation, Configuration [X] achieved the best performance with:
- BLEU score: [Score]
- ROUGE-L score: [Score]

Detailed analysis and visualizations can be found in the Technical Report.

## Example

**Input (Complex):**
```
The patient, a 67-year-old male with a history of hypertension, type 2 diabetes, and chronic kidney disease, presented with shortness of breath and chest tightness. An echocardiogram revealed reduced ejection fraction suggestive of systolic heart failure.
```

**Output (Simplified):**
```
A 67-year-old man with high blood pressure, type 2 diabetes, and kidney problems came to the hospital with trouble breathing and chest pain. Tests showed his heart wasn't pumping blood properly, which means heart failure.
```

## Contributing

Contributions to improve the model or extend its functionality are welcome. Please feel free to submit a pull request or open an issue.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- The T5 model was developed by Google Research
- Implementation uses the Hugging Face Transformers library
- Data from the GEM benchmark's Cochrane Simplification dataset

## Contact

For any questions or feedback, please contact [your email or GitHub username].
