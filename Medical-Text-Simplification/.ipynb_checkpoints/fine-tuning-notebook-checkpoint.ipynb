{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Scouting Report Generator - Fine-Tuning\n",
    "\n",
    "This notebook walks through the process of fine-tuning a language model to generate NBA scouting reports from player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn torch transformers datasets nltk rouge-score tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM, AutoTokenizer\n",
    ")\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import random\n",
    "import logging\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "First, we'll create a custom dataset class for handling NBA player stats and scouting reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ScoutingReportDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NBA player stats and scouting reports.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_file (str): Path to the JSON file containing the dataset\n",
    "            tokenizer: Hugging Face tokenizer\n",
    "            max_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load data\n",
    "        with open(data_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} examples from {data_file}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Create prompt with player stats\n",
    "        prompt = f\"Generate a comprehensive NBA scouting report for the following player stats:\\n\\n\"\n",
    "        prompt += f\"Name: {item['player_name']}\\n\"\n",
    "        prompt += f\"Position: {item['position']}\\n\"\n",
    "        prompt += f\"Age: {item['age']}\\n\"\n",
    "        prompt += f\"Height: {item['height']}\\n\"\n",
    "        prompt += f\"Weight: {item['weight']} lbs\\n\"\n",
    "        prompt += f\"Team: {item['team']}\\n\"\n",
    "        prompt += f\"Season Stats: {item['season_stats']}\\n\\n\"\n",
    "        prompt += f\"Scouting Report:\"\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        encodings = self.tokenizer(\n",
    "            prompt, \n",
    "            item['scouting_report'], \n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encodings.input_ids[0]\n",
    "        attention_mask = encodings.attention_mask[0]\n",
    "        \n",
    "        # Find where the prompt ends and response begins\n",
    "        prompt_encodings = self.tokenizer(prompt, truncation=True, max_length=self.max_length)\n",
    "        prompt_length = len(prompt_encodings.input_ids)\n",
    "        \n",
    "        # Create labels with -100 for prompt (will be ignored in loss calculation)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_length] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"prompt\": prompt,\n",
    "            \"report\": item['scouting_report']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define functions to prepare and split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_dataset(stats_file, reports_file, output_file):\n",
    "    \"\"\"\n",
    "    Prepare dataset by combining player stats and scouting reports.\n",
    "    \n",
    "    Args:\n",
    "        stats_file (str): Path to the CSV file containing player stats\n",
    "        reports_file (str): Path to the CSV file containing scouting reports\n",
    "        output_file (str): Path to save the combined dataset\n",
    "    \"\"\"\n",
    "    # Load player stats\n",
    "    player_stats = pd.read_csv(stats_file)\n",
    "    \n",
    "    # Load scouting reports\n",
    "    scouting_reports = pd.read_csv(reports_file)\n",
    "    \n",
    "    # Merge the dataframes on player ID/name\n",
    "    combined_data = pd.merge(\n",
    "        player_stats, \n",
    "        scouting_reports, \n",
    "        on='player_id',  # Adjust based on your actual column names\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"Combined dataset has {len(combined_data)} rows\")\n",
    "    print(f\"Sample columns: {combined_data.columns.tolist()[:10]}...\")\n",
    "    \n",
    "    # Convert to the format needed for fine-tuning\n",
    "    dataset = []\n",
    "    for _, row in combined_data.iterrows():\n",
    "        # Format season stats as a string\n",
    "        season_stats = (\n",
    "            f\"PPG: {row['pts']}, RPG: {row['reb']}, \"\n",
    "            f\"APG: {row['ast']}, SPG: {row['stl']}, \"\n",
    "            f\"BPG: {row['blk']}, FG%: {row['fg_pct']}, \"\n",
    "            f\"3P%: {row['fg3_pct']}, FT%: {row['ft_pct']}\"\n",
    "        )\n",
    "        \n",
    "        dataset.append({\n",
    "            'player_id': row['player_id'],\n",
    "            'player_name': row['player_name'],\n",
    "            'position': row['position'],\n",
    "            'age': row['age'],\n",
    "            'height': row['height'],\n",
    "            'weight': row['weight'],\n",
    "            'team': row['team'],\n",
    "            'season_stats': season_stats,\n",
    "            'scouting_report': row['scouting_report']\n",
    "        })\n",
    "    \n",
    "    # Save the processed dataset\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Processed dataset saved to {output_file}\")\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(data_file, output_dir, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        data_file (str): Path to the JSON file containing the dataset\n",
    "        output_dir (str): Directory to save the split datasets\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        val_size (float): Proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(data_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Split into train and temp (val + test)\n",
    "    train_data, temp_data = train_test_split(data, test_size=(test_size + val_size), random_state=42)\n",
    "    \n",
    "    # Split temp into val and test\n",
    "    relative_test_size = test_size / (test_size + val_size)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=relative_test_size, random_state=42)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the splits\n",
    "    with open(os.path.join(output_dir, 'train.json'), 'w') as f:\n",
    "        json.dump(train_data, f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'val.json'), 'w') as f:\n",
    "        json.dump(val_data, f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'test.json'), 'w') as f:\n",
    "        json.dump(test_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset split into {len(train_data)} training, {len(val_data)} validation, and {len(test_data)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the data preparation steps. You'll need to provide your own data files or generate synthetic data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up directories\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Define paths\n",
    "stats_file = os.path.join(data_dir, \"player_stats.csv\")\n",
    "reports_file = os.path.join(data_dir, \"scouting_reports.csv\")\n",
    "combined_file = os.path.join(data_dir, \"combined_dataset.json\")\n",
    "\n",
    "# Check if files exist\n",
    "if os.path.exists(stats_file) and os.path.exists(reports_file):\n",
    "    # Prepare dataset\n",
    "    prepare_dataset(stats_file, reports_file, combined_file)\n",
    "    \n",
    "    # Split dataset\n",
    "    split_dataset(combined_file, data_dir)\n",
    "else:\n",
    "    print(f\"Please place your player stats CSV at {stats_file} and scouting reports at {reports_file}\")\n",
    "    print(\"For the assignment, you'll need to collect or generate this data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def select_model(model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Select and load a pre-trained model for fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def fine_tune_model(\n",
    "    model, tokenizer, \n",
    "    train_file, val_file, \n",
    "    output_dir, \n",
    "    learning_rate=5e-5,\n",
    "    batch_size=8,\n",
    "    num_epochs=3,\n",
    "    warmup_steps=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune the model on the training data.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        train_file (str): Path to the training data\n",
    "        val_file (str): Path to the validation data\n",
    "        output_dir (str): Directory to save the fine-tuned model\n",
    "        learning_rate (float): Learning rate for fine-tuning\n",
    "        batch_size (int): Batch size for training\n",
    "        num_epochs (int): Number of training epochs\n",
    "        warmup_steps (int): Number of warmup steps\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = ScoutingReportDataset(train_file, tokenizer)\n",
    "    val_dataset = ScoutingReportDataset(val_file, tokenizer)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, 'logs'),\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting fine-tuning\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model_save_path = os.path.join(output_dir, 'final_model')\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"Fine-tuned model saved to {model_save_path}\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def hyperparameter_optimization(\n",
    "    model_name, \n",
    "    train_file, \n",
    "    val_file, \n",
    "    output_dir,\n",
    "    configs=[\n",
    "        {\"learning_rate\": 5e-5, \"batch_size\": 4, \"num_epochs\": 3},\n",
    "        {\"learning_rate\": 2e-5, \"batch_size\": 8, \"num_epochs\": 5},\n",
    "        {\"learning_rate\": 1e-5, \"batch_size\": 16, \"num_epochs\": 10}\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization to find the best training configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model to use\n",
    "        train_file (str): Path to the training data\n",
    "        val_file (str): Path to the validation data\n",
    "        output_dir (str): Directory to save the models and results\n",
    "        configs (list): List of hyperparameter configurations to try\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Try each configuration\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"Testing configuration {i+1}/{len(configs)}: {config}\")\n",
    "        \n",
    "        # Create subdirectory for this configuration\n",
    "        config_dir = os.path.join(output_dir, f\"config_{i+1}\")\n",
    "        os.makedirs(config_dir, exist_ok=True)\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = select_model(model_name)\n",
    "        \n",
    "        # Fine-tune with this configuration\n",
    "        fine_tune_model(\n",
    "            model, tokenizer,\n",
    "            train_file, val_file,\n",
    "            config_dir,\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            num_epochs=config[\"num_epochs\"]\n",
    "        )\n",
    "        \n",
    "        # Load fine-tuned model and evaluate on validation set\n",
    "        model = AutoModelForCausalLM.from_pretrained(os.path.join(config_dir, 'final_model'))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(os.path.join(config_dir, 'final_model'))\n",
    "        \n",
    "        # Create validation dataset\n",
    "        val_dataset = ScoutingReportDataset(val_file, tokenizer)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TrainingArguments(\n",
    "                output_dir=config_dir,\n",
    "                per_device_eval_batch_size=8\n",
    "            ),\n",
    "            eval_dataset=val_dataset\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Store results\n",
    "        results[f\"config_{i+1}\"] = {\n",
    "            \"config\": config,\n",
    "            \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "            \"perplexity\": np.exp(eval_results[\"eval_loss\"])\n",
    "        }\n",
    "        \n",
    "        print(f\"Configuration {i+1} results: {results[f'config_{i+1}']}\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_config_id = min(results, key=lambda k: results[k][\"eval_loss\"])\n",
    "    best_config = results[best_config_id]\n",
    "    \n",
    "    print(f\"Best configuration: {best_config_id}\")\n",
    "    print(f\"Config details: {best_config['config']}\")\n",
    "    print(f\"Eval loss: {best_config['eval_loss']}\")\n",
    "    print(f\"Perplexity: {best_config['perplexity']}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, 'hyperparameter_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    config_ids = list(results.keys())\n",
    "    losses = [results[c][\"eval_loss\"] for c in config_ids]\n",
    "    perplexities = [results[c][\"perplexity\"] for c in config_ids]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.bar(config_ids, losses)\n",
    "    ax1.set_xlabel('Configuration')\n",
    "    ax1.set_ylabel('Validation Loss')\n",
    "    ax1.set_title('Validation Loss by Configuration')\n",
    "    \n",
    "    ax2.bar(config_ids, perplexities)\n",
    "    ax2.set_xlabel('Configuration')\n",
    "    ax2.set_ylabel('Perplexity')\n",
    "    ax2.set_title('Perplexity by Configuration')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'hyperparameter_results.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return best_config_id, best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the model and run hyperparameter optimization if we have data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up model directory\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "hyperparam_dir = os.path.join(model_dir, \"hyperparameter_search\")\n",
    "\n",
    "# Check if we have data files\n",
    "train_file = os.path.join(data_dir, \"train.json\")\n",
    "val_file = os.path.join(data_dir, \"val.json\")\n",
    "\n",
    "if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "    # For this notebook, we'll use simplified hyperparameter configs to save time\n",
    "    # In a real project, you'd want more extensive testing\n",
    "    simplified_configs = [\n",
    "        {\"learning_rate\": 5e-5, \"batch_size\": 4, \"num_epochs\": 2},  # Reduced epochs for demo\n",
    "        {\"learning_rate\": 2e-5, \"batch_size\": 8, \"num_epochs\": 3}\n",
    "    ]\n",
    "    \n",
    "    # Run hyperparameter optimization\n",
    "    best_config_id, best_config = hyperparameter_optimization(\n",
    "        \"gpt2\",  # You can change to other models like \"distilgpt2\" for faster training\n",
    "        train_file,\n",
    "        val_file,\n",
    "        hyperparam_dir,\n",
    "        configs=simplified_configs\n",
    "    )\n",
    "    \n",
    "    # Save the best config info for reference\n",
    "    with open(os.path.join(model_dir, \"best_config.json\"), 'w') as f:\n",
    "        json.dump({\n",
    "            \"best_config_id\": best_config_id,\n",
    "            \"config_details\": best_config\n",
    "        }, f, indent=2)\n",
    "else:\n",
    "    print(f\"Training data not found at {train_file} or {val_file}\")\n",
    "    print(\"Please complete the data preparation steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model Training\n",
    "\n",
    "Now, let's train our final model using the best hyperparameters we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if we have identified a best configuration\n",
    "best_config_file = os.path.join(model_dir, \"best_config.json\")\n",
    "\n",
    "if os.path.exists(best_config_file):\n",
    "    # Load best config\n",
    "    with open(best_config_file, 'r') as f:\n",
    "        best_config_info = json.load(f)\n",
    "    \n",
    "    best_config_id = best_config_info[\"best_config_id\"]\n",
    "    best_config = best_config_info[\"config_details\"]\n",
    "    \n",
    "    print(f\"Training final model with best configuration: {best_config['config']}\")\n",
    "    \n",
    "    # Set up final model directory\n",
    "    final_model_dir = os.path.join(model_dir, \"final_model\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Load fresh model\n",
    "    model, tokenizer = select_model(\"gpt2\")\n",
    "    \n",
    "    # Train on full training set with best hyperparameters\n",
    "    fine_tune_model(\n",
    "        model, tokenizer,\n",
    "        train_file, val_file,\n",
    "        final_model_dir,\n",
    "        learning_rate=best_config[\"config\"][\"learning_rate\"],\n",
    "        batch_size=best_config[\"config\"][\"batch_size\"],\n",
    "        num_epochs=best_config[\"config\"][\"num_epochs\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Final model training complete!\")\n",
    "else:\n",
    "    print(\"No best configuration found. Please run hyperparameter optimization first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_report(model, tokenizer, player_stats, max_length=1024):\n",
    "    \"\"\"\n",
    "    Generate a scouting report for a player based on their stats.\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        player_stats (dict): Dictionary containing player statistics\n",
    "        max_length (int): Maximum length of generated text\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated scouting report\n",
    "    \"\"\"\n",
    "    # Create prompt\n",
    "    prompt = f\"Generate a comprehensive NBA scouting report for the following player stats:\\n\\n\"\n",
    "    prompt += f\"Name: {player_stats['player_name']}\\n\"\n",
    "    prompt += f\"Position: {player_stats['position']}\\n\"\n",
    "    prompt += f\"Age: {player_stats['age']}\\n\"\n",
    "    prompt += f\"Height: {player_stats['height']}\\n\"\n",
    "    prompt += f\"Weight: {player_stats['weight']} lbs\\n\"\n",
    "    prompt += f\"Team: {player_stats['team']}\\n\"\n",
    "    prompt += f\"Season Stats: {player_stats['season_stats']}\\n\\n\"\n",
    "    prompt += f\"Scouting Report:\"\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate report\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"] if \"attention_mask\" in inputs else None,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return generated text\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the scouting report part (remove the prompt)\n",
    "    report = generated_text[len(prompt):]\n",
    "    \n",
    "    return report.strip()\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_file, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate the fine-tuned model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        test_file (str): Path to the test data\n",
    "        output_file (str): Path to save the evaluation results\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    with open(test_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Set up metrics\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Evaluate each example\n",
    "    for item in tqdm(test_data, desc=\"Evaluating model\"):\n",
    "        # Generate report\n",
    "        generated_report = generate_report(model, tokenizer, item)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = scorer.score(item['scouting_report'], generated_report)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        reference = nltk.word_tokenize(item['scouting_report'].lower())\n",
    "        candidate = nltk.word_tokenize(generated_report.lower())\n",
    "        bleu = sentence_bleu([reference], candidate)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'player_name': item['player_name'],\n",
    "            'position': item['position'],\n",
    "            'actual_report': item['scouting_report'],\n",
    "            'generated_report': generated_report,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "            'bleu': bleu\n",
    "        })\n",
    "        \n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        bleu_scores.append(bleu)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_results = {\n",
    "        'avg_rouge1': np.mean(rouge1_scores),\n",
    "        'avg_rouge2': np.mean(rouge2_scores),\n",
    "        'avg_rougeL': np.mean(rougeL_scores),\n",
    "        'avg_bleu': np.mean(bleu_scores)\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'individual_results': results,\n",
    "            'average_scores': avg_results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"Evaluation completed\")\n",
    "    print(f\"Average ROUGE-1: {avg_results['avg_rouge1']:.4f}\")\n",
    "    print(f\"Average ROUGE-2: {avg_results['avg_rouge2']:.4f}\")\n",
    "    print(f\"Average ROUGE-L: {avg_results['avg_rougeL']:.4f}\")\n",
    "    print(f\"Average BLEU: {avg_results['avg_bleu']:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU']\n",
    "    scores = [\n",
    "        avg_results['avg_rouge1'],\n",
    "        avg_results['avg_rouge2'],\n",
    "        avg_results['avg_rougeL'],\n",
    "        avg_results['avg_bleu']\n",
    "    ]\n",
    "    \n",
    "    plt.bar(metrics, scores)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Evaluation Metrics')\n",
    "    \n",
    "    for i, v in enumerate(scores):\n",
    "        plt.text(i, v + 0.05, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.savefig(os.path.join(os.path.dirname(output_file), 'evaluation_metrics.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return results, avg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate our final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up results directory\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Check if we have a final model and test data\n",
    "final_model_dir = os.path.join(model_dir, \"final_model\")\n",
    "test_file = os.path.join(data_dir, \"test.json\")\n",
    "\n",
    "if os.path.exists(final_model_dir) and os.path.exists(test_file):\n",
    "    # Load final model\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_model_dir)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results, avg_scores = evaluate_model(\n",
    "        model, tokenizer,\n",
    "        test_file,\n",
    "        os.path.join(results_dir, \"evaluation_results.json\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"Final model not found at {final_model_dir} or test data not found at {test_file}\")\n",
    "    print(\"Please complete the model training steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Baseline\n",
    "\n",
    "Let's compare our fine-tuned model with the base model (without fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_with_baseline(base_model_name, finetuned_model_dir, test_file, output_file):\n",
    "    \"\"\"\n",
    "    Compare the fine-tuned model with the base model.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name (str): Name of the base pre-trained model\n",
    "        finetuned_model_dir (str): Directory containing the fine-tuned model\n",
    "        test_file (str): Path to the test data\n",
    "        output_file (str): Path to save the comparison results\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    base_model, base_tokenizer = select_model(base_model_name)\n",
    "    \n",
    "    finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_dir)\n",
    "    finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
    "    \n",
    "    # Load test data\n",
    "    with open(test_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Sample a subset for comparison (to save time)\n",
    "    sample_size = min(5, len(test_data))\n",
    "    sample_data = random.sample(test_data, sample_size)\n",
    "    \n",
    "    # Set up metrics\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    base_results = []\n",
    "    finetuned_results = []\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for item in tqdm(sample_data, desc=\"Comparing models\"):\n",
    "        # Generate reports\n",
    "        base_report = generate_report(base_model, base_tokenizer, item)\n",
    "        finetuned_report = generate_report(finetuned_model, finetuned_tokenizer, item)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        base_rouge = scorer.score(item['scouting_report'], base_report)\n",
    "        finetuned_rouge = scorer.score(item['scouting_report'], finetuned_report)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        reference = nltk.word_tokenize(item['scouting_report'].lower())\n",
    "        \n",
    "        base_candidate = nltk.word_tokenize(base_report.lower())\n",
    "        base_bleu = sentence_bleu([reference], base_candidate)\n",
    "        \n",
    "        finetuned_candidate = nltk.word_tokenize(finetuned_report.lower())\n",
    "        finetuned_bleu = sentence_bleu([reference], finetuned_candidate)\n",
    "        \n",
    "        # Store results\n",
    "        base_results.append({\n",
    "            'player_name': item['player_name'],\n",
    "            'report': base_report,\n",
    "            'rouge1': base_rouge['rouge1'].fmeasure,\n",
    "            'rouge2': base_rouge['rouge2'].fmeasure,\n",
    "            'rougeL': base_rouge['rougeL'].fmeasure,\n",
    "            'bleu': base_bleu\n",
    "        })\n",
    "        \n",
    "        finetuned_results.append({\n",
    "            'player_name': item['player_name'],\n",
    "            'report': finetuned_report,\n",
    "            'rouge1': finetuned_rouge['rouge1'].fmeasure,\n",
    "            'rouge2': finetuned_rouge['rouge2'].fmeasure,\n",
    "            'rougeL': finetuned_rouge['rougeL'].fmeasure,\n",
    "            'bleu': finetuned_bleu\n",
    "        })\n",
    "    \n",
    "    # Calculate average scores\n",
    "    base_avg = {\n",
    "        'avg_rouge1': np.mean([r['rouge1'] for r in base_results]),\n",
    "        'avg_rouge2': np.mean([r['rouge2'] for r in base_results]),\n",
    "        'avg_rougeL': np.mean([r['rougeL'] for r in base_results]),\n",
    "        'avg_bleu': np.mean([r['bleu'] for r in base_results])\n",
    "    }\n",
    "    \n",
    "    finetuned_avg = {\n",
    "        'avg_rouge1': np.mean([r['rouge1'] for r in finetuned_results]),\n",
    "        'avg_rouge2': np.mean([r['rouge2'] for r in finetuned_results]),\n",
    "        'avg_rougeL': np.mean([r['rougeL'] for r in finetuned_results]),\n",
    "        'avg_bleu': np.mean([r['bleu'] for r in finetuned_results])\n",
    "    }\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    improvements = {\n",
    "        metric: ((finetuned_avg[f'avg_{metric}'] - base_avg[f'avg_{metric}']) / base_avg[f'avg_{metric}'] * 100) \n",
    "        if base_avg[f'avg_{metric}'] > 0 else float('inf')\n",
    "        for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu']\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'base_model': {\n",
    "                'individual_results': base_results,\n",
    "                'average_scores': base_avg\n",
    "            },\n",
    "            'finetuned_model': {\n",
    "                'individual_results': finetuned_results,\n",
    "                'average_scores': finetuned_avg\n",
    "            },\n",
    "            'improvements': improvements\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU']\n",
    "    base_scores = [\n",
    "        base_avg['avg_rouge1'],\n",
    "        base_avg['avg_rouge2'],\n",
    "        base_avg['avg_rougeL'],\n",
    "        base_avg['avg_bleu']\n",
    "    ]\n",
    "    \n",
    "    finetuned_scores = [\n",
    "        finetuned_avg['avg_rouge1'],\n",
    "        finetuned_avg['avg_rouge2'],\n",
    "        finetuned_avg['avg_rougeL'],\n",
    "        finetuned_avg['avg_bleu']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    rects1 = ax.bar(x - width/2, base_scores, width, label='Base Model')\n",
    "    rects2 = ax.bar(x + width/2, finetuned_scores, width, label='Fine-tuned Model')\n",
    "    \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Comparison: Base vs. Fine-tuned')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    \n",
    "    for i, v in enumerate(base_scores):\n",
    "        ax.text(i - width/2, v + 0.05, f'{v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "    for i, v in enumerate(finetuned_scores):\n",
    "        ax.text(i + width/2, v + 0.05, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(os.path.dirname(output_file), 'model_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Model comparison completed\")\n",
    "    print(f\"Base model average scores: {base_avg}\")\n",
    "    print(f\"Fine-tuned model average scores: {finetuned_avg}\")\n",
    "    print(f\"Improvements: {improvements}\")\n",
    "    \n",
    "    # Print a sample comparison\n",
    "    if len(sample_data) > 0:\n",
    "        print(\"\\nSample Comparison:\")\n",
    "        print(f\"Player: {sample_data[0]['player_name']}\")\n",
    "        print(\"\\nActual Report:\")\n",
    "        print(sample_data[0]['scouting_report'])\n",
    "        print(\"\\nBase Model Report:\")\n",
    "        print(base_results[0]['report'])\n",
    "        print(\"\\nFine-tuned Model Report:\")\n",
    "        print(finetuned_results[0]['report'])\n",
    "    \n",
    "    return base_results, finetuned_results, improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if we have a final model and test data\n",
    "if os.path.exists(final_model_dir) and os.path.exists(test_file):\n",
    "    # Compare with baseline\n",
    "    base_results, finetuned_results, improvements = compare_with_baseline(\n",
    "        \"gpt2\",  # Base model\n",
    "        final_model_dir,  # Fine-tuned model\n",
    "        test_file,\n",
    "        os.path.join(results_dir, \"model_comparison.json\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"Final model not found at {final_model_dir} or test data not found at {test_file}\")\n",
    "    print(\"Please complete the model training steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with a Custom Example\n",
    "\n",
    "Let's test our fine-tuned model with a custom player example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a sample player\n",
    "sample_player = {\n",
    "    'player_name': \"LeBron James\",\n",
    "    'position': \"SF\",\n",
    "    'age': 37,\n",
    "    'height': \"6'9\\\"\",\n",
    "    'weight': 250,\n",
    "    'team': \"Los Angeles Lakers\",\n",
    "    'season_stats': \"PPG: 30.3, RPG: 8.2, APG: 6.2, SPG: 1.3, BPG: 1.1, FG%: 0.524, 3P%: 0.359, FT%: 0.756\"\n",
    "}\n",
    "\n",
    "# Check if we have a final model\n",
    "if os.path.exists(final_model_dir):\n",
    "    # Load final model\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_model_dir)\n",
    "    \n",
    "    # Generate report\n",
    "    print(\"Generating scouting report for LeBron James...\")\n",
    "    report = generate_report(model, tokenizer, sample_player)\n",
    "    \n",
    "    print(\"\\nGenerated Scouting Report:\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(f\"Final model not found at {final_model_dir}\")\n",
    "    print(\"Please complete the model training steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model for Streamlit App\n",
    "\n",
    "Now, let's ensure our model is saved in the correct format for the Streamlit app to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if we have a final model\n",
    "if os.path.exists(final_model_dir):\n",
    "    # The model is already saved in the correct format for Streamlit to use\n",
    "    print(f\"Model is saved at {final_model_dir} and ready for use in the Streamlit app.\")\n",
    "    print(\"\\nTo start the Streamlit app, run the following command in your terminal:\")\n",
    "    print(\"streamlit run app.py\")\n",
    "    \n",
    "    # Create a simple text file with the model path for the app to reference\n",
    "    with open(\"model_path.txt\", \"w\") as f:\n",
    "        f.write(final_model_dir)\n",
    "    \n",
    "    print(\"\\nModel path saved to model_path.txt for the app to reference.\")\n",
    "else:\n",
    "    print(f\"Final model not found at {final_model_dir}\")\n",
    "    print(\"Please complete the model training steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've completed the entire fine-tuning pipeline for our NBA Scouting Report Generator:\n",
    "\n",
    "1. Dataset preparation\n",
    "2. Model selection\n",
    "3. Hyperparameter optimization\n",
    "4. Final model training\n",
    "5. Model evaluation\n",
    "6. Comparison with the baseline model\n",
    "7. Testing with a custom example\n",
    "8. Saving the model for the Streamlit app\n",
    "\n",
    "The fine-tuned model is now ready to be used in the Streamlit application, which will provide a user-friendly interface for generating NBA scouting reports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
